---
title: "P3-5"
subtitle: "Times Series Forecasting"
author: "Philipp Altenbach, Taoufik Brinis, Ronny Grieder, Ryan Kreis"
date: today
date-format: long
format:
  # html:
  #   theme: zephyr
  #   code-fold: true
  #   page-layout: full
  pdf:
    code-overflow: wrap        # Prevent code overflow
    number-sections: true      # Section Numbers
    code-line-numbers: true    # Show line numbers in code
    fig-width: 7               # Adjust figure width
    fig-height: 5              # Adjust figure height
    fig-pos: "H"               # Keep figures in place
    fig-align: center          # Align figures at the center
    tbl-cap-location: top      # Table captions at the top
    tbl-colwidths: auto        # Allow tables to auto-adjust width
    geometry: a4paper, margin=1.0in # A4 page with slightly reduced margins
execute:
  echo: true                   # Show code in output
  warning: false               # Suppress warnings
  message: false               # Suppress messages
editor: visual
---

```{r setup, include=FALSE}

# install required packages if needed
if (!require("fpp3")) install.packages("fpp3")
if (!require("data.table")) install.packages("data.table")
if (!require("dplyr")) install.packages("dplyr")
if (!require("DataExplorer")) install.packages("DataExplorer")
if (!require("tidyr")) install.packages("tidyr")
if (!require("ggplot2")) install.packages("ggplot2")
if (!require("scales")) install.packages("scales")


# loading required packages
library(fpp3)

```

```{r Prelimiaries}
# Loading our underlying timeseries using ID: CEU1021000001
mySeries <- us_employment %>%
  filter(Series_ID == "CEU1021000001")

# Autoplot and first inspection of mySeries
View(mySeries)
autoplot(mySeries, Employed)

# Checking for NA
mySeries %>% filter(is.na(Employed)) %>% head()

# Removing NA
mySeries <- mySeries %>%
  filter(!is.na(Employed))

# Saving series on disk
saveRDS(mySeries, file = "CEU1021000001_series.rds")

```

# 1. (0.5 Points) Extract the Training Set

```{r extract training data}

# Load the saved time series
mySeries <- readRDS("CEU1021000001_series.rds")

# multiplying by 0.8 to get training data
n_total <- nrow(mySeries)
n_train <- floor(0.8 * n_total)

# storing train
train <- mySeries %>% 
  slice(1:n_train)
```

# 2. (0.5 points) Train your 1st Benchmark Model

```{r 1st bechnmark model}

# Train seasonal naive model
model_benchmark1 <- train %>%
  model(
    drift = RW(Employed ~ drift())
  )

print(model_benchmark1)

# Inspect fitted values using augment()
fitted_benchmark1 <- model_benchmark1 %>%
  augment()

```

# 3. (2 points) Evaluate the Model Fit of Your 1st Benchmark Model

## a. Create a time plot ’Actual vs. Fitted’:

```{r actual-vs-fitted}
# Plot actual vs. fitted values
fitted_benchmark1 %>%
  autoplot(Employed) +
  autolayer(fitted_benchmark1, .fitted, color = "red", linetype = "dashed") +
  labs(title = "Actual vs. Fitted Values – Drift Model",
       y = "Employed",
       x = "Date") +
  theme_minimal()
```

**Are the fitted values close to the actual values?**

-   Yes, the fitted values are close to the actual values.

**Are they systematically too high or too low?**

-   They do not seam to be systematically too high or too low

**If yes, where does it come from? (Consider how your benchmark model works!)**

-   Answer was no. The drift method seems to be a good fit.

## b. Create a scatter plot ’Actual vs. Fitted’:

```{r scatter plot Actual vs. Fitted}

fitted_benchmark1 %>%
  ggplot(aes(x = .fitted, y = Employed)) +
  geom_point(alpha = 0.5, color = "darkblue") +
  geom_abline(slope = 1, intercept = 0, linetype = 2, color = "red") +
  labs(title = "Actual vs. Fitted – Drift Model",
       x = "Fitted Values",
       y = "Actual Values") +
  theme_minimal()

```

**Are the points close to the identity line?**

-   Yes, the points generally cluster around the identity line, indicating a reasonably good overall fit.

**Are they systematically too high or too low?**

-   There are no strong systematic deviations, but some points diverge and form outliers

**If yes, where does it come from? (Relate your answer to what you saw in the time plot.)**

-   Outliers might occur because the drift model captures long-term trends but fails to adjust for short-term fluctuations.

## c. Perform residual diagnostics to inspect the model fit:

```{r residual diagnostics for 1st benchmark model}
# Residual diagnostics for the drift model
model_benchmark1 %>%
  gg_tsresiduals()

```

**Are the residuals auto-correlated? How do you decide that based on the plots?**

-   Yes, the residuals show signs of autocorrelation, as several spikes in the ACF plot exceed the significance bounds.

**Do the residuals have zero mean? How do you decide that based on the plots?**

-   Yes, the histogram of residuals is centered around zero, and the time plot of residuals fluctuates symmetrically around the zero line

**What do these results tell you about your model?**

-   The model captures the general trend but fails to fully account for short-term dependencies, indicating room for improvement beyond a drift-based benchmark. There might still be some information the model was not able to catch.

## d. Double-check your results from (c).

```{r ljung-box-and-residual-mean}
# Ljung-Box test for residual autocorrelation
# Ljung-Box test on residuals with lag = 24
fitted_benchmark1 %>%
  features(.resid, ljung_box, lag = 24, dof = 1)

# Residual mean
mean(fitted_benchmark1$.resid, na.rm = TRUE)


```

**Does the Ljung-Box test result support your conclusions from (c)?**

-   Yes, the Ljung-Box test confirms significant autocorrelation in the residuals (p-value ≈ 7.98e-06), which supports the earlier observation from the ACF plot.

**Does the residual mean result support your conclusions from (c)?**

-   No, not really since the residual mean is effectively zero but not quite (≈ –2.97e-14), confirming that autocorrelation is present within the residuals.

# 4. (1.5 points) Evaluate the Point Forecast Accuracy of your 1st Benchmark Model

```{r forecast-drift}

forecast_length <- n_total - n_train
# Forecast for 149 months ahead
fc_benchmark1 <- model_benchmark1 %>%
  forecast(h = forecast_length)

# Time plot of forecast vs actual
fc_benchmark1 %>%
  autoplot(mySeries) +
  labs(title = "Forecast vs Actual – Drift Model",
       y = "Employed",
       x = "Date") +
  theme_minimal()


```

```{r}
# Calculate accuracy on full data
fc_benchmark1 %>%
  accuracy(mySeries)
```

## Interpretation of accuracy metrics

The root mean squared error (RMSE) is approximately 105, which is substantial relative to the scale of the series and indicates notable deviations from actual values during the forecast period. This level of error suggests that while the model captures the general trend, it fails to account for finer structural shifts or turning points. The mean absolute percentage error (MAPE) of 9.3% indicates that the model's forecasts are off by about 9% on average, which is moderate for a benchmark model but clearly insufficient for precise forecasting in policy or economic analysis.

# 5. (0.5 points) Evaluate the Point Forecast Uncertainty of Your 1st Benchmark Model

```{r residual-diagnostics-uncertainty}
# Residual diagnostics for evaluating forecast uncertainty
model_benchmark1 %>%
  gg_tsresiduals()

```

## Are the residuals homoscedastic? (That is, do they have constant variance?)

-   No, the residuals are not entirely homoscedastic; a few sharp spikes around 1980 indicate temporary variance inflation.

## Are the residuals normally distributed?

-   Roughly yes, the histogram of residuals appears bell-shaped and centered around zero but it also shows some asymmetry and heavy tails

```{r prediction-intervals}
# Display prediction intervals (80% and 95%) for forecast
fc_benchmark1 %>%
  hilo()

```

## Does it make sense to include these prediction intervals in your model evaluation? Why/why not?

-   No, it does not make sense to rely on these prediction intervals for model evaluation because the drift model has poor residual diagnostics (autocorrelation, heteroscedasticity), which undermines the reliability of the uncertainty estimates.
