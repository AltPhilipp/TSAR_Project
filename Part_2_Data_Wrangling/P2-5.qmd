---
title: "TSAR Project Assignment Part 2"
subtitle: "Data Wrangling with dplyr and tidyr"
author: "Philipp Altenbach, Taoufik Brinis, Ronny Grieder, Ryan Kreis"
date: today
date-format: long
format:
  # html:
  #   theme: zephyr
  #   code-fold: true
  #   page-layout: full
  pdf:
    code-overflow: wrap        # Prevent code overflow
    number-sections: true      # Section Numbers
    code-line-numbers: true    # Show line numbers in code
    fig-width: 7               # Adjust figure width
    fig-height: 5              # Adjust figure height
    fig-pos: "H"               # Keep figures in place
    fig-align: center          # Align figures at the center
    tbl-cap-location: top      # Table captions at the top
    tbl-colwidths: auto        # Allow tables to auto-adjust width
    geometry: a4paper, margin=1.0in # A4 page with slightly reduced margins
execute:
  echo: true                   # Show code in output
  warning: false               # Suppress warnings
  message: false               # Suppress messages
editor: visual
---

```{r setup, include=FALSE}
# install the required packages if needed
# need to be commented because myLCdata file must not always be created again
# use only myLCdata <- fread("Data/myLCdata.csv")

# install required packages if needed
if (!require("here")) install.packages("here")
if (!require("data.table")) install.packages("data.table")
if (!require("dplyr")) install.packages("dplyr")
if (!require("DataExplorer")) install.packages("DataExplorer")
if (!require("tidyr")) install.packages("tidyr")
if (!require("ggplot2")) install.packages("ggplot2")
if (!require("scales")) install.packages("scales")


# loading required packages
library(here)
library(data.table)
library(dplyr)
library(tidyr)
library(DataExplorer)
library(ggplot2)
library(scales)


TSAR_ID <- 5

# Please make sure to set WD to the same folder as .qmd file.
data_path <- "Data/myLCdata.csv"
rds_path <- "dirtyfy.rds"

# Load data and dirtyfy function
myLCdata <- fread(data_path)
dirtyfy_function <- readRDS(rds_path)
myLCdata_dirty <- dirtyfy_function(myLCdata, TSAR_ID)

# # OPTIONAL: Original project-based paths using `here()` (if needed)
# data_path <- file.path(here(), "Part_1_EDA/Data", "myLCdata.csv")
# rds_path <- file.path(here(), "Part_2_Data_Wrangling", "dirtyfy.rds")
# png_path <- file.path(here(), "Part_2_Data_Wrangling", "Missing_Data_Profile.png")
# 
# myLCdata <- fread(data_path)
# dirtyfy_function <- readRDS(rds_path)
# myLCdata_dirty <- dirtyfy_function(myLCdata, TSAR_ID)
```

# Introduction

The purpose of this report is to demonstrate data wrangling techniques using `dplyr` and `tidyr` within the R environment. This assignment builds upon the previously created private data set from Project 1 and applies additional steps to intentionally "dirtyfy" the data. Through exploratory data analysis (EDA) and systematic detection of special values (missing values, NaN, and outliers), this report aims to visualize data quality issues present within our data set.

# Task 1

## Creating a report using the DataExplorer's function `create_report()` .

```{r report}
# create_report(myLCdata_dirty) # Generate EDA Report
```
Do not execute `create_report()` within the Quarto document. Use a direct execution in the console instead.

## Screenshot of the `Missing Data Profile` overview

![Missing Data Profile generated by DataExplorer. The plot shows the percentage of missing values (NAs) per attribute.](Missing_Data_Profile.png){#fig-missing}

As shown in @fig-missing, the Missing Data Profile generated by DataExplorer provides an overview of the percentage of missing values across the attributes. This visualization helps us to quickly identify variables with significant amounts of missing data.

# Task 2

We decided to split Task 2 into four steps for better readability and an increased learning experience.

## Counting specials

Our first goal was to create a new data frame object called `char_specials`. `char_specials` is a **data frame** with **one row**, where each **column** contains the **count** of a specific type of special value, as indicated in the project description (e.g., `NA`, `"n/a"`, etc.), for each **character column** in the original `myLCdata_dirty`. However, since our data frame from project one, `myLCdata,` only contains numerical values/attributes, we expect the counts to be zero or empty for the special character columns. We found out that the result will be a empty **tibble,** or in other words, an **empty data frame.**

```{r char_specials}
char_specials <- myLCdata_dirty %>%
  summarise(
    across(
      where(is.character),                    # Apply to all character columns
      list(
        na_count = ~sum(is.na(.)),            # Count NAs
        n_a_count = ~sum(. == "n/a"),         # Count "n/a" strings
        space_count = ~sum(. == " "),         # Count single spaces
        empty_count = ~sum(. == "")           # Count empty strings
      ),
      .names = "{.col}_{.fn}"                 # Create clear output column names
    )
  )

# Testing what char_specials really is now
# print(typeof(char_specials))
# ncol(char_specials) 
# nrow(char_specials) 
# is.data.frame(char_specials)  
str(char_specials)
```

Following that, we applied the same approach for the numerical values contained within the `myLCdata_dirty` data frame.

```{r num_specials}
num_specials <- myLCdata_dirty %>%
  summarise(
    across(
      where(is.numeric),
      list(
        na_count = ~sum(is.na(.)),   # Count NAs
        nan_count = ~sum(is.nan(.))  # Count NaNs
      ),
      .names = "{.col}_{.fn}"
    )
  )

# print(typeof(num_specials))
# ncol(num_specials) 
# nrow(num_specials) 
# is.data.frame(num_specials)  
str(num_specials) # df structure
```
## Identifying outliers and relative calculation

Secondly, we focused on the outliers. For that, we could recycle the logic or approach we did in step one. Since we already proofed again that our the underlying data set does not hold any character values, we solely focused on the `is.numeric` analysis.

```{r outlier_count}
outlier_specials <- myLCdata_dirty %>%
  summarise(
    across(
      where(is.numeric), # Across all numeric values
      ~length(boxplot.stats(.)$out), # Get number of outliers of each variable
      .names = "{.col}_outlier_count"
    )
  )
```

The above code created a new data frame called `outlier_specials` that holds the following values.

```{r head}
head(outlier_specials)
is.data.frame(outlier_specials)
```
For each attribute the **count of outliers** was created, based on the `boxplot.stats(col)$out`logic, and stored in the above mentioned df.

When comparing the above values with our plots created for Task 1, the numbers appear to be reasonable.

## Tidying data

Our next goals was to create a **tidy** data frame showing the **percentage** of:

-   `NA`

-   `NaN`

-   `outliers`

for each **numeric column** in `myLCdata_dirty`.

First, we can bind our two previously created data frames `num_specials` and `outlier_specials` together. Again, as justified earlier, we do not have to do an analysis for characteristic values.

```{r bind_cols}
# creating a new data frame that binds the data together
num_all_specials <- bind_cols(num_specials, outlier_specials)
# head(num_all_specials) # Might lead to overflow in PDF
```

Subsequently, we needed to tidy our data frame `num_all_specials` using the `pivot_longer()` function from **tidyr.** This step was actually quite helpful to understand the `pivot_longer()` logic in a "real" use case.

```{r pivot_longer}

num_all_specials_long <- num_all_specials %>%
  pivot_longer(
    cols = everything(),
    names_to = c("variable", "type"),
    names_sep = "_(?=[^_]+$)",  # Split at last underscore
    values_to = "count"
  )

# df is now restructured with columns now as rows (longer logic)
print(num_all_specials_long)
```

Next, we now needed to convert the **counts to percentage**. For this step we made use of the introduced `mutate()` function.

```{r relative_counts}
num_all_specials_long <- num_all_specials_long %>% # Apply following functions to df
  mutate(
    percentage = count / nrow(myLCdata_dirty) # First, compute percentage using `count`
  ) %>%
  separate(variable, into = c("attribute", "type"), sep = "_(?=[^_]+$)") %>%
  select(attribute, type, percentage)  # Then drop `count` to keep it clean

# print(nrow(myLCdata_dirty))
print(num_all_specials_long)
```

At this point, our data frame is **not fully tidy**, as it contains multiple separate rows representing different attributes of the same variable. Therefore, as a final step before visualization, we need to ensure the data conforms to a **wide tidy structure**. For example, the `int_rate` variable currently appears in two distinct rows, each representing the counts and percentages of missing values (i.e., NAs and NaNs) separately.

```{r pivot_wide}

# Pivot to wide format to get one row per attribute
num_all_specials_tidy <- num_all_specials_long %>%
  pivot_wider(
    names_from = type,
    values_from = percentage,
    names_prefix = "perc_"
  )

print(num_all_specials_tidy)
```

## Plotting the results

We now wanted to bring our tidy data frame to life with a plot using **ggplot2.**

```{r ggplot}
#| label: fig-special
#| fig-cap: "Percentage of special values (NA, NaN, outliers) per numerical attribute in `myLCdata_dirty`. The plot displays the proportion of each special value type across the attributes using `geom_col`. Facetting is applied to separate the categories, allowing for clear comparison between the different types of special values."

ggplot(num_all_specials_long, aes(x = attribute, y = percentage)) +
  geom_col(width = 0.7, fill = "steelblue") +
  geom_text(
    aes(label = scales::percent(percentage, accuracy = 0.1)),
    hjust = -0.2,                      
    size = 3.5,                        # Font size of the labels
    color = "red"
  ) +
  coord_flip() +
  scale_y_continuous(
    labels = percent_format(), 
    breaks = seq(0, 1, by = 0.1),
    limits = c(0, 0.7)
  ) +
  facet_wrap(~type, ncol = 1, scales = "free_y") +
  labs(
    title = "Percentage of special values in the numerical attributes of 'myLCdata'",
    x = NULL,
    y = NULL
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
    strip.text = element_text(face = "bold", size = 11),
    panel.grid.major.y = element_line(color = "white"),  # Re-enable Y grid lines
    panel.grid.major.x = element_line(color = "white"),  # Subtle X grid lines
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "grey90", color = NA)
  )
```
@fig-special illustrates the proportion of special values across the numerical attributes of the dirty data set.

# Conclusion

In this assignment, data wrangling techniques were successfully applied to identify and quantify special values such as missing values, NaNs, and outliers in the modified version of our data set. The systematic use of `dplyr`, `tidyr`, and `ggplot2` enabled efficient data summarization and visualization. The results highlight the importance of thoroughly inspecting data quality before conducting further analysis.
